"""Integration tests for pinyinize system.

These tests verify end-to-end functionality with realistic scenarios.
"""

from __future__ import annotations

import json
import tempfile
import unittest
from pathlib import Path

from pinyinize.core import PinyinizeOptions, pinyinize
from pinyinize.resources import PinyinResources


class TestAcceptanceCriteria(unittest.TestCase):
    """Tests based on the acceptance criteria from CLAUDE.md."""

    def _create_complete_data(self, root: Path) -> None:
        """Create complete test data covering acceptance criteria."""
        # word.json - comprehensive word list
        word_items = [
            {"word": "ç»†è¯´", "pinyin": "xÃ¬ shuÅ"},
            {"word": "é“¶è¡Œ", "pinyin": "yÃ­n hÃ¡ng"},
            {"word": "è¡Œé•¿", "pinyin": "hÃ¡ng zhÇŽng"},
            {"word": "é‡æ–°", "pinyin": "chÃ³ng xÄ«n"},
            {"word": "è¥ä¸š", "pinyin": "yÃ­ng yÃ¨"},
            {"word": "å¾—åˆ°", "pinyin": "dÃ© dÃ o"},
            {"word": "ç­”æ¡ˆ", "pinyin": "dÃ¡ Ã n"},
            {"word": "å¾—åŽ»", "pinyin": "dÄ›i qÃ¹"},
            {"word": "åŒè¡Œ", "pinyin": "tÃ³ng hÃ¡ng"},
            {"word": "è¡Œèµ°", "pinyin": "xÃ­ng zÇ’u"},
            {"word": "é‡è¦", "pinyin": "zhÃ²ng yÃ o"},
            {"word": "é‡å¤", "pinyin": "chÃ³ng fÃ¹"},
            {"word": "å¿«ä¹", "pinyin": "kuÃ i lÃ¨"},
            {"word": "éŸ³ä¹", "pinyin": "yÄ«n yuÃ¨"},
            {"word": "ç›®çš„", "pinyin": "mÃ¹ dÃ¬"},
            {"word": "çš„ç¡®", "pinyin": "dÃ­ quÃ¨"},
        ]
        root.joinpath("word.json").write_text(
            '{"word": "ç»†è¯´", "pinyin": "xÃ¬ shuÅ"},\n'
            '{"word": "é“¶è¡Œ", "pinyin": "yÃ­n hÃ¡ng"},\n'
            '{"word": "è¡Œé•¿", "pinyin": "hÃ¡ng zhÇŽng"},\n'
            '{"word": "é‡æ–°", "pinyin": "chÃ³ng xÄ«n"},\n'
            '{"word": "è¥ä¸š", "pinyin": "yÃ­ng yÃ¨"},\n'
            '{"word": "å¾—åˆ°", "pinyin": "dÃ© dÃ o"},\n'
            '{"word": "ç­”æ¡ˆ", "pinyin": "dÃ¡ Ã n"},\n'
            '{"word": "å¾—åŽ»", "pinyin": "dÄ›i qÃ¹"},\n'
            '{"word": "åŒè¡Œ", "pinyin": "tÃ³ng hÃ¡ng"},\n'
            '{"word": "è¡Œèµ°", "pinyin": "xÃ­ng zÇ’u"},\n'
            '{"word": "é‡è¦", "pinyin": "zhÃ²ng yÃ o"},\n'
            '{"word": "é‡å¤", "pinyin": "chÃ³ng fÃ¹"},\n'
            '{"word": "å¿«ä¹", "pinyin": "kuÃ i lÃ¨"},\n'
            '{"word": "éŸ³ä¹", "pinyin": "yÄ«n yuÃ¨"},\n'
            '{"word": "ç›®çš„", "pinyin": "mÃ¹ dÃ¬"},\n'
            '{"word": "çš„ç¡®", "pinyin": "dÃ­ quÃ¨"},\n',
            encoding="utf-8",
        )

        # char_base.json - character mappings
        char_items = [
            {"index": 1, "char": "ç»†", "pinyin": ["xÃ¬"]},
            {"index": 2, "char": "è¯´", "pinyin": ["shuÅ"]},
            {"index": 3, "char": "é“¶", "pinyin": ["yÃ­n"]},
            {"index": 4, "char": "è¡Œ", "pinyin": ["xÃ­ng", "hÃ¡ng"]},
            {"index": 5, "char": "é•¿", "pinyin": ["chÃ¡ng", "zhÇŽng"]},
            {"index": 6, "char": "é‡", "pinyin": ["zhÃ²ng", "chÃ³ng"]},
            {"index": 7, "char": "æ–°", "pinyin": ["xÄ«n"]},
            {"index": 8, "char": "è¥", "pinyin": ["yÃ­ng"]},
            {"index": 9, "char": "ä¸š", "pinyin": ["yÃ¨"]},
            {"index": 10, "char": "å¾—", "pinyin": ["de", "dÃ©", "dÄ›i"]},
            {"index": 11, "char": "åˆ°", "pinyin": ["dÃ o"]},
            {"index": 12, "char": "ç­”", "pinyin": ["dÃ¡"]},
            {"index": 13, "char": "æ¡ˆ", "pinyin": ["Ã n"]},
            {"index": 14, "char": "åŽ»", "pinyin": ["qÃ¹"]},
            {"index": 15, "char": "ä»–", "pinyin": ["tÄ"]},
            {"index": 16, "char": "æˆ‘", "pinyin": ["wÇ’"]},
            {"index": 17, "char": "ä½ ", "pinyin": ["nÇ"]},
            {"index": 18, "char": "çš„", "pinyin": ["de", "dÃ­", "dÃ¬"]},
            {"index": 19, "char": "åŒ", "pinyin": ["tÃ³ng"]},
            {"index": 20, "char": "èµ°", "pinyin": ["zÇ’u"]},
            {"index": 21, "char": "è¦", "pinyin": ["yÃ o"]},
            {"index": 22, "char": "å¤", "pinyin": ["fÃ¹"]},
            {"index": 23, "char": "å¿«", "pinyin": ["kuÃ i"]},
            {"index": 24, "char": "ä¹", "pinyin": ["lÃ¨", "yuÃ¨"]},
            {"index": 25, "char": "éŸ³", "pinyin": ["yÄ«n"]},
            {"index": 26, "char": "ç›®", "pinyin": ["mÃ¹"]},
            {"index": 27, "char": "çš„", "pinyin": ["dÃ­", "dÃ¬", "de"]},
            {"index": 28, "char": "ç¡®", "pinyin": ["quÃ¨"]},
            {"index": 29, "char": "ä¸­", "pinyin": ["zhÅng", "zhÃ²ng"]},
            {"index": 30, "char": "å›½", "pinyin": ["guÃ³"]},
        ]
        root.joinpath("char_base.json").write_text(
            '{"index": 1, "char": "ç»†", "pinyin": ["xÃ¬"]},\n'
            '{"index": 2, "char": "è¯´", "pinyin": ["shuÅ"]},\n'
            '{"index": 3, "char": "é“¶", "pinyin": ["yÃ­n"]},\n'
            '{"index": 4, "char": "è¡Œ", "pinyin": ["xÃ­ng", "hÃ¡ng"]},\n'
            '{"index": 5, "char": "é•¿", "pinyin": ["chÃ¡ng", "zhÇŽng"]},\n'
            '{"index": 6, "char": "é‡", "pinyin": ["zhÃ²ng", "chÃ³ng"]},\n'
            '{"index": 7, "char": "æ–°", "pinyin": ["xÄ«n"]},\n'
            '{"index": 8, "char": "è¥", "pinyin": ["yÃ­ng"]},\n'
            '{"index": 9, "char": "ä¸š", "pinyin": ["yÃ¨"]},\n'
            '{"index": 10, "char": "å¾—", "pinyin": ["de", "dÃ©", "dÄ›i"]},\n'
            '{"index": 11, "char": "åˆ°", "pinyin": ["dÃ o"]},\n'
            '{"index": 12, "char": "ç­”", "pinyin": ["dÃ¡"]},\n'
            '{"index": 13, "char": "æ¡ˆ", "pinyin": ["Ã n"]},\n'
            '{"index": 14, "char": "åŽ»", "pinyin": ["qÃ¹"]},\n'
            '{"index": 15, "char": "ä»–", "pinyin": ["tÄ"]},\n'
            '{"index": 16, "char": "æˆ‘", "pinyin": ["wÇ’"]},\n'
            '{"index": 17, "char": "ä½ ", "pinyin": ["nÇ"]},\n'
            '{"index": 18, "char": "çš„", "pinyin": ["de", "dÃ­", "dÃ¬"]},\n'
            '{"index": 19, "char": "åŒ", "pinyin": ["tÃ³ng"]},\n'
            '{"index": 20, "char": "èµ°", "pinyin": ["zÇ’u"]},\n'
            '{"index": 21, "char": "è¦", "pinyin": ["yÃ o"]},\n'
            '{"index": 22, "char": "å¤", "pinyin": ["fÃ¹"]},\n'
            '{"index": 23, "char": "å¿«", "pinyin": ["kuÃ i"]},\n'
            '{"index": 24, "char": "ä¹", "pinyin": ["lÃ¨", "yuÃ¨"]},\n'
            '{"index": 25, "char": "éŸ³", "pinyin": ["yÄ«n"]},\n'
            '{"index": 26, "char": "ç›®", "pinyin": ["mÃ¹"]},\n'
            '{"index": 27, "char": "çš„", "pinyin": ["dÃ­", "dÃ¬", "de"]},\n'
            '{"index": 28, "char": "ç¡®", "pinyin": ["quÃ¨"]},\n'
            '{"index": 29, "char": "ä¸­", "pinyin": ["zhÅng", "zhÃ²ng"]},\n'
            '{"index": 30, "char": "å›½", "pinyin": ["guÃ³"]},\n',
            encoding="utf-8",
        )

        # polyphone.json - polyphone definitions
        root.joinpath("polyphone.json").write_text(
            json.dumps([
                {"index": 1, "char": "è¡Œ", "pinyin": ["xÃ­ng", "hÃ¡ng"]},
                {"index": 2, "char": "é•¿", "pinyin": ["chÃ¡ng", "zhÇŽng"]},
                {"index": 3, "char": "é‡", "pinyin": ["zhÃ²ng", "chÃ³ng"]},
                {"index": 4, "char": "å¾—", "pinyin": ["de", "dÃ©", "dÄ›i"]},
                {"index": 5, "char": "çš„", "pinyin": ["de", "dÃ­", "dÃ¬"]},
                {"index": 6, "char": "ä¹", "pinyin": ["lÃ¨", "yuÃ¨"]},
                {"index": 7, "char": "ä¸­", "pinyin": ["zhÅng", "zhÃ²ng"]},
            ], ensure_ascii=False) + "\n",
            encoding="utf-8",
        )

        # polyphone_disambig.json - disambiguation rules
        root.joinpath("polyphone_disambig.json").write_text(
            json.dumps({
                "schema": "complete_test",
                "thresholds": {"min_support": 5, "min_prob": 0.85, "min_margin": 0.15},
                "items": [
                    {
                        "char": "è¡Œ",
                        "candidates": ["xÃ­ng", "hÃ¡ng"],
                        "default": "xÃ­ng",
                        "contexts": {
                            "pos=NOUN|ner=O": {"best": "hÃ¡ng", "p": 0.88, "p2": 0.12, "n": 100},
                            "pos=VERB|ner=O": {"best": "xÃ­ng", "p": 0.90, "p2": 0.10, "n": 120},
                        },
                    },
                    {
                        "char": "é•¿",
                        "candidates": ["chÃ¡ng", "zhÇŽng"],
                        "default": "chÃ¡ng",
                        "contexts": {
                            "pos=ADJ|ner=O": {"best": "chÃ¡ng", "p": 0.92, "p2": 0.08, "n": 150},
                            "pos=NOUN|ner=O": {"best": "zhÇŽng", "p": 0.87, "p2": 0.13, "n": 110},
                        },
                    },
                    {
                        "char": "é‡",
                        "candidates": ["zhÃ²ng", "chÃ³ng"],
                        "default": "zhÃ²ng",
                        "contexts": {
                            "pos=ADJ|ner=O": {"best": "zhÃ²ng", "p": 0.91, "p2": 0.09, "n": 130},
                            "pos=ADV|ner=O": {"best": "chÃ³ng", "p": 0.89, "p2": 0.11, "n": 95},
                        },
                    },
                    {
                        "char": "å¾—",
                        "candidates": ["de", "dÃ©", "dÄ›i"],
                        "default": "de",
                        "contexts": {
                            "pos=PART|ner=O": {"best": "de", "p": 0.95, "p2": 0.03, "n": 200},
                            "pos=VERB|ner=O": {"best": "dÃ©", "p": 0.88, "p2": 0.08, "n": 85},
                            "pos=AUX|ner=O": {"best": "dÄ›i", "p": 0.86, "p2": 0.10, "n": 70},
                        },
                    },
                ],
            }, ensure_ascii=False) + "\n",
            encoding="utf-8",
        )

        root.joinpath("overrides.json").write_text(
            json.dumps({"schema_version": 1, "rules": []}, ensure_ascii=False) + "\n",
            encoding="utf-8",
        )
        root.joinpath("lexicon.json").write_text(
            json.dumps({"schema_version": 1, "items": []}, ensure_ascii=False) + "\n",
            encoding="utf-8",
        )

    def test_criterion_1_basic(self) -> None:
        """Test criterion 1: ç»†è¯´ -> xÃ¬shuÅ."""
        with tempfile.TemporaryDirectory() as td:
            root = Path(td)
            self._create_complete_data(root)
            resources = PinyinResources.load_from_dir(root)
            opts = PinyinizeOptions(resources=resources)

            results = pinyinize("ç»†è¯´", opts)
            self.assertTrue(any(r.output_text == "xÃ¬shuÅ" for r in results))

    def test_criterion_2_bank_director(self) -> None:
        """Test criterion 2: é“¶è¡Œè¡Œé•¿é‡æ–°è¥ä¸š -> yÃ­nhÃ¡ng hÃ¡ngzhÇŽng chÃ³ngxÄ«n yÃ­ngyÃ¨."""
        with tempfile.TemporaryDirectory() as td:
            root = Path(td)
            self._create_complete_data(root)
            resources = PinyinResources.load_from_dir(root)
            opts = PinyinizeOptions(resources=resources)

            results = pinyinize("é“¶è¡Œè¡Œé•¿é‡æ–°è¥ä¸š", opts)
            self.assertTrue(any(r.output_text == "yÃ­nhÃ¡ng hÃ¡ngzhÇŽng chÃ³ngxÄ«n yÃ­ngyÃ¨" for r in results))

    def test_criterion_3_de_polyphone(self) -> None:
        """Test criterion 3: ä»–å¾—åŽ»å¾—åˆ°ç­”æ¡ˆ -> tÄ dÄ›iqÃ¹ dÃ©dÃ o dÃ¡Ã n."""
        with tempfile.TemporaryDirectory() as td:
            root = Path(td)
            self._create_complete_data(root)
            resources = PinyinResources.load_from_dir(root)
            opts = PinyinizeOptions(resources=resources)

            results = pinyinize("ä»–å¾—åŽ»å¾—åˆ°ç­”æ¡ˆ", opts)
            self.assertTrue(any(r.output_text == "tÄ dÄ›iqÃ¹ dÃ©dÃ o dÃ¡Ã n" for r in results))

    def test_criterion_4_mixed_content(self) -> None:
        """Test criterion 4: Mixed content preservation."""
        with tempfile.TemporaryDirectory() as td:
            root = Path(td)
            self._create_complete_data(root)
            resources = PinyinResources.load_from_dir(root)
            opts = PinyinizeOptions(resources=resources)

            results = pinyinize("ç»†è¯´OpenAIçš„API v2.0ï¼šhttps://openai.com", opts)
            self.assertTrue(any("https://openai.com" in r.output_text for r in results))
            self.assertTrue(any("OpenAI" in r.output_text for r in results))
            self.assertTrue(any("v2.0" in r.output_text for r in results))

    def test_url_character_exact(self) -> None:
        """Test that URL is preserved character-by-character."""
        with tempfile.TemporaryDirectory() as td:
            root = Path(td)
            self._create_complete_data(root)
            resources = PinyinResources.load_from_dir(root)
            opts = PinyinizeOptions(resources=resources)

            url = "https://openai.com/api/v2?key=value"
            results = pinyinize(f"è®¿é—®{url}å³å¯", opts)
            self.assertTrue(any(url in r.output_text for r in results))

    def test_report_has_decision_sources(self) -> None:
        """Test report tracks decision sources."""
        with tempfile.TemporaryDirectory() as td:
            root = Path(td)
            self._create_complete_data(root)
            resources = PinyinResources.load_from_dir(root)
            opts = PinyinizeOptions(resources=resources)

            results = pinyinize("ç»†è¯´", opts)
            for result in results:
                report = result.report
                self.assertTrue(len(report["tokens"]) > 0)
                for token in report["tokens"]:
                    self.assertIn("char_decisions", token)
                    for decision in token["char_decisions"]:
                        self.assertIn("resolved_by", decision)
                        valid_sources = [
                            "word", "char_base", "polyphone_disambig",
                            "override", "llm_double_check", "user", "fallback", "unknown"
                        ]
                        self.assertIn(decision["resolved_by"], valid_sources)


class TestRealWorldScenarios(unittest.TestCase):
    """Real-world usage scenarios."""

    def _create_scenario_data(self, root: Path) -> None:
        """Create data for scenario tests."""
        root.joinpath("word.json").write_text(
            '{"word": "ä¸­å›½é“¶è¡Œ", "pinyin": "zhÅng guÃ³ yÃ­n hÃ¡ng"},\n'
            '{"word": "å·¥å•†é“¶è¡Œ", "pinyin": "gÅng shÄng yÃ­n hÃ¡ng"},\n'
            '{"word": "å»ºè®¾é“¶è¡Œ", "pinyin": "jiÃ n shÃ¨ yÃ­n hÃ¡ng"},\n'
            '{"word": "å†œä¸šé“¶è¡Œ", "pinyin": "nÃ³ng yÃ¨ yÃ­n hÃ¡ng"},\n'
            '{"word": "ä¸­å›½äººæ°‘é“¶è¡Œ", "pinyin": "zhÅng guÃ³ rÃ©n mÃ­n yÃ­n hÃ¡ng"},\n'
            '{"word": "æ€»ç»ç†", "pinyin": "zÇ’ng jÄ«ng lÇ"},\n'
            '{"word": "è‘£äº‹é•¿", "pinyin": "dÇ’ng shÃ¬ zhÇŽng"},\n'
            '{"word": "éƒ¨é•¿", "pinyin": "bÃ¹ zhÇŽng"},\n'
            '{"word": "æ ¡é•¿", "pinyin": "xiÃ o zhÇŽng"},\n'
            '{"word": "å¸‚é•¿", "pinyin": "shÃ¬ zhÇŽng"},\n',
            encoding="utf-8",
        )

        root.joinpath("char_base.json").write_text(
            '{"index": 1, "char": "ä¸­", "pinyin": ["zhÅng", "zhÃ²ng"]},\n'
            '{"index": 2, "char": "å›½", "pinyin": ["guÃ³"]},\n'
            '{"index": 3, "char": "é“¶", "pinyin": ["yÃ­n"]},\n'
            '{"index": 4, "char": "è¡Œ", "pinyin": ["xÃ­ng", "hÃ¡ng"]},\n'
            '{"index": 5, "char": "å·¥", "pinyin": ["gÅng"]},\n'
            '{"index": 6, "char": "å•†", "pinyin": ["shÄng"]},\n'
            '{"index": 7, "char": "å»º", "pinyin": ["jiÃ n"]},\n'
            '{"index": 8, "char": "è®¾", "pinyin": ["shÃ¨"]},\n'
            '{"index": 9, "char": "å†œ", "pinyin": ["nÃ³ng"]},\n'
            '{"index": 10, "char": "ä¸š", "pinyin": ["yÃ¨"]},\n'
            '{"index": 11, "char": "äºº", "pinyin": ["rÃ©n"]},\n'
            '{"index": 12, "char": "æ°‘", "pinyin": ["mÃ­n"]},\n'
            '{"index": 13, "char": "æ€»", "pinyin": ["zÇ’ng"]},\n'
            '{"index": 14, "char": "ç»", "pinyin": ["jÄ«ng"]},\n'
            '{"index": 15, "char": "ç†", "pinyin": ["lÇ"]},\n'
            '{"index": 16, "char": "è‘£", "pinyin": ["dÇ’ng"]},\n'
            '{"index": 17, "char": "äº‹", "pinyin": ["shÃ¬"]},\n'
            '{"index": 18, "char": "é•¿", "pinyin": ["chÃ¡ng", "zhÇŽng"]},\n'
            '{"index": 19, "char": "éƒ¨", "pinyin": ["bÃ¹"]},\n'
            '{"index": 20, "char": "æ ¡", "pinyin": ["xiÃ o", "jiÃ o"]},\n'
            '{"index": 21, "char": "å¸‚", "pinyin": ["shÃ¬"]},\n',
            encoding="utf-8",
        )

        root.joinpath("polyphone.json").write_text(
            json.dumps([
                {"index": 1, "char": "ä¸­", "pinyin": ["zhÅng", "zhÃ²ng"]},
                {"index": 2, "char": "è¡Œ", "pinyin": ["xÃ­ng", "hÃ¡ng"]},
                {"index": 3, "char": "é•¿", "pinyin": ["chÃ¡ng", "zhÇŽng"]},
                {"index": 4, "char": "æ ¡", "pinyin": ["xiÃ o", "jiÃ o"]},
            ], ensure_ascii=False) + "\n",
            encoding="utf-8",
        )

        root.joinpath("polyphone_disambig.json").write_text(
            json.dumps({
                "schema": "scenario_test",
                "thresholds": {"min_support": 5, "min_prob": 0.85, "min_margin": 0.15},
                "items": [],
            }, ensure_ascii=False) + "\n",
            encoding="utf-8",
        )

        root.joinpath("overrides.json").write_text(
            json.dumps({"schema_version": 1, "rules": []}, ensure_ascii=False) + "\n",
            encoding="utf-8",
        )
        root.joinpath("lexicon.json").write_text(
            json.dumps({"schema_version": 1, "items": []}, ensure_ascii=False) + "\n",
            encoding="utf-8",
        )

    def test_bank_names(self) -> None:
        """Test major Chinese bank names."""
        with tempfile.TemporaryDirectory() as td:
            root = Path(td)
            self._create_scenario_data(root)
            resources = PinyinResources.load_from_dir(root)
            opts = PinyinizeOptions(resources=resources)

            test_cases = [
                ("ä¸­å›½é“¶è¡Œ", "zhÅngguÃ³yÃ­nhÃ¡ng"),
                ("å·¥å•†é“¶è¡Œ", "gÅngshÄngyÃ­nhÃ¡ng"),
                ("å»ºè®¾é“¶è¡Œ", "jiÃ nshÃ¨yÃ­nhÃ¡ng"),
                ("å†œä¸šé“¶è¡Œ", "nÃ³ngyÃ¨yÃ­nhÃ¡ng"),
            ]

            for input_text, expected in test_cases:
                results = pinyinize(input_text, opts)
                self.assertTrue(any(r.output_text == expected for r in results), f"Failed for {input_text}")

    def test_job_titles(self) -> None:
        """Test job title pronunciations."""
        with tempfile.TemporaryDirectory() as td:
            root = Path(td)
            self._create_scenario_data(root)
            resources = PinyinResources.load_from_dir(root)
            opts = PinyinizeOptions(resources=resources)

            test_cases = [
                ("æ€»ç»ç†", "zÇ’ngjÄ«nglÇ"),
                ("è‘£äº‹é•¿", "dÇ’ngshÃ¬zhÇŽng"),
                ("éƒ¨é•¿", "bÃ¹zhÇŽng"),
                ("æ ¡é•¿", "xiÃ ozhÇŽng"),
                ("å¸‚é•¿", "shÃ¬zhÇŽng"),
            ]

            for input_text, expected in test_cases:
                results = pinyinize(input_text, opts)
                self.assertTrue(any(r.output_text == expected for r in results), f"Failed for {input_text}")


class TestEdgeCases(unittest.TestCase):
    """Edge case tests."""

    def _create_minimal_data(self, root: Path) -> None:
        root.joinpath("word.json").write_text("", encoding="utf-8")
        root.joinpath("char_base.json").write_text(
            '{"index": 1, "char": "æµ‹", "pinyin": ["cÃ¨"]},\n'
            '{"index": 2, "char": "è¯•", "pinyin": ["shÃ¬"]},\n',
            encoding="utf-8",
        )
        root.joinpath("polyphone.json").write_text("[]\n", encoding="utf-8")
        root.joinpath("polyphone_disambig.json").write_text(
            json.dumps({
                "schema": "edge_test",
                "thresholds": {"min_support": 5, "min_prob": 0.85, "min_margin": 0.15},
                "items": [],
            }, ensure_ascii=False) + "\n",
            encoding="utf-8",
        )
        root.joinpath("overrides.json").write_text(
            json.dumps({"schema_version": 1, "rules": []}, ensure_ascii=False) + "\n",
            encoding="utf-8",
        )
        root.joinpath("lexicon.json").write_text(
            json.dumps({"schema_version": 1, "items": []}, ensure_ascii=False) + "\n",
            encoding="utf-8",
        )

    def test_empty_input(self) -> None:
        """Test empty input."""
        with tempfile.TemporaryDirectory() as td:
            root = Path(td)
            self._create_minimal_data(root)
            resources = PinyinResources.load_from_dir(root)
            opts = PinyinizeOptions(resources=resources)

            results = pinyinize("", opts)
            self.assertTrue(any(r.output_text == "" for r in results))

    def test_only_spaces(self) -> None:
        """Test input with only spaces."""
        with tempfile.TemporaryDirectory() as td:
            root = Path(td)
            self._create_minimal_data(root)
            resources = PinyinResources.load_from_dir(root)
            opts = PinyinizeOptions(resources=resources)

            results = pinyinize("   ", opts)
            self.assertTrue(any(r.output_text == "   " for r in results))

    def test_only_punctuation(self) -> None:
        """Test input with only punctuation."""
        with tempfile.TemporaryDirectory() as td:
            root = Path(td)
            self._create_minimal_data(root)
            resources = PinyinResources.load_from_dir(root)
            opts = PinyinizeOptions(resources=resources)

            results = pinyinize("ï¼Œã€‚ï¼ï¼Ÿ", opts)
            self.assertTrue(any(r.output_text == "ï¼Œã€‚ï¼ï¼Ÿ" for r in results))

    def test_emoji(self) -> None:
        """Test input with emoji."""
        with tempfile.TemporaryDirectory() as td:
            root = Path(td)
            self._create_minimal_data(root)
            resources = PinyinResources.load_from_dir(root)
            opts = PinyinizeOptions(resources=resources)

            results = pinyinize("æµ‹è¯•ðŸ˜€", opts)
            self.assertTrue(any("ðŸ˜€" in r.output_text for r in results))

    def test_numbers_and_chinese(self) -> None:
        """Test numbers mixed with Chinese."""
        with tempfile.TemporaryDirectory() as td:
            root = Path(td)
            self._create_minimal_data(root)
            resources = PinyinResources.load_from_dir(root)
            opts = PinyinizeOptions(resources=resources)

            results = pinyinize("2024å¹´æµ‹è¯•", opts)
            self.assertTrue(any("2024" in r.output_text for r in results))
            self.assertTrue(any("cÃ¨" in r.output_text for r in results))


if __name__ == "__main__":
    unittest.main()
